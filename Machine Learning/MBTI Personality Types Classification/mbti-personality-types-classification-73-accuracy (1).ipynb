{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4260039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T18:59:21.485250Z",
     "iopub.status.busy": "2021-10-27T18:59:21.483999Z",
     "iopub.status.idle": "2021-10-27T18:59:23.117051Z",
     "shell.execute_reply": "2021-10-27T18:59:23.115790Z",
     "shell.execute_reply.started": "2021-10-27T18:28:11.673942Z"
    },
    "papermill": {
     "duration": 1.653282,
     "end_time": "2021-10-27T18:59:23.117254",
     "exception": false,
     "start_time": "2021-10-27T18:59:21.463972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615dc163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T18:59:23.151204Z",
     "iopub.status.busy": "2021-10-27T18:59:23.150361Z",
     "iopub.status.idle": "2021-10-27T18:59:23.159899Z",
     "shell.execute_reply": "2021-10-27T18:59:23.160402Z",
     "shell.execute_reply.started": "2021-10-27T18:28:16.615575Z"
    },
    "papermill": {
     "duration": 0.02976,
     "end_time": "2021-10-27T18:59:23.160584",
     "exception": false,
     "start_time": "2021-10-27T18:59:23.130824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing tools\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "str_punc = string.punctuation\n",
    "\n",
    "engstopwords = stopwords.words(\"english\")\n",
    "engstopwordsV2 = re.sub('[' + re.escape(string.punctuation) + ']', '',\n",
    "                        ' '.join(engstopwords)).split()\n",
    "\n",
    "engstopwords = set(engstopwords).union(set(engstopwordsV2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591d5578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T18:59:23.200278Z",
     "iopub.status.busy": "2021-10-27T18:59:23.189433Z",
     "iopub.status.idle": "2021-10-27T18:59:23.205483Z",
     "shell.execute_reply": "2021-10-27T18:59:23.205954Z",
     "shell.execute_reply.started": "2021-10-27T18:28:19.431232Z"
    },
    "papermill": {
     "duration": 0.03291,
     "end_time": "2021-10-27T18:59:23.206164",
     "exception": false,
     "start_time": "2021-10-27T18:59:23.173254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to lemmatize a word using the three types: adjective, verb, noun\n",
    "def lemmatize_all_types(word):\n",
    "    word = wnl.lemmatize(word, 'a')\n",
    "    word = wnl.lemmatize(word, 'v')\n",
    "    word = wnl.lemmatize(word, 'n')\n",
    "    return word\n",
    "\n",
    "# Function to clean text\n",
    "def clean(text):\n",
    "    # Remove URLs from text\n",
    "    text = re.sub(\"http.*?([ ]|\\|\\|\\||$)\", \"\", text).lower()\n",
    "    url_regex = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    text = re.sub(url_regex, \"\", text)\n",
    "\n",
    "    # Remove specific punctuation (usually associated with a word)\n",
    "    text = re.sub(r'(:|;).', \" \", text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = re.sub('['+re.escape(str_punc)+']',\" \",  text)\n",
    "    \n",
    "    # Remove parantheses, brackets\n",
    "    text = re.sub('(\\[|\\()*\\d+(\\]|\\))*', ' ', text)\n",
    "    \n",
    "    # Remove string marks\n",
    "    text = re.sub('[’‘“\\.”…–]', '', text)\n",
    "    text = re.sub('[^(\\w|\\s)]', '', text)\n",
    "    text = re.sub('(gt|lt)', '', text)\n",
    "    \n",
    "    #Check that each word is not stopword, and lemmatize it\n",
    "    text = list(map(lemmatize_all_types, text.split()))\n",
    "    text = [word for word in text if (word not in engstopwords)]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# Convert personality type into a dominant cognitive function\n",
    "def letters_to_functions(personality_type):\n",
    "    translator = {\n",
    "            'ENxP': 'Ne',\n",
    "            'INxJ': 'Ni',\n",
    "            'ESxP': 'Se',\n",
    "            'ISxJ': 'Si',\n",
    "            'ExTJ': 'Te',\n",
    "            'IxTP': 'Ti',\n",
    "            'IxFP': 'Fi',\n",
    "            'ExFJ': 'Fe',\n",
    "            \n",
    "            'xNxx': 'N',\n",
    "            'xSxx': 'S',\n",
    "            'xxTx' : 'T',\n",
    "            'xxFx': 'F',\n",
    "            'Ixxx':'I',\n",
    "            'Exxx':'E',\n",
    "            }\n",
    "    return translator[personality_type]\n",
    "\n",
    "# Convert a Cognitive Functions Stack into Personality Type\n",
    "def functions_to_letters(functions):\n",
    "    translator = {\n",
    "            'NiTe': 'INTJ',\n",
    "            'NiFe': 'INFJ',\n",
    "            'NeTi': 'ENTP',\n",
    "            'NeFi': 'ENFP',\n",
    "            'SiTe': 'ISTJ',\n",
    "            'SiFe': 'ISFJ',\n",
    "            'SeTi': 'ESTP',\n",
    "            'SeFi': 'ESFP',\n",
    "            'TeNi': 'ENTJ',\n",
    "            'FeNi': 'ENFJ',\n",
    "            'TiNe': 'INTP',\n",
    "            'FiNe': 'INFP',\n",
    "            'TeSi': 'ESTJ',\n",
    "            'FeSi': 'ESFJ',\n",
    "            'TiSe': 'ISTP',\n",
    "            'FiSe': 'ISFP',\n",
    "            }\n",
    "    return translator[functions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1639310e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T18:59:23.243897Z",
     "iopub.status.busy": "2021-10-27T18:59:23.234543Z",
     "iopub.status.idle": "2021-10-27T18:59:23.246336Z",
     "shell.execute_reply": "2021-10-27T18:59:23.246852Z",
     "shell.execute_reply.started": "2021-10-27T18:28:22.569131Z"
    },
    "papermill": {
     "duration": 0.028194,
     "end_time": "2021-10-27T18:59:23.247079",
     "exception": false,
     "start_time": "2021-10-27T18:59:23.218885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cognitive Functions meta-information\n",
    "cf_info = {\n",
    "        \"Ni\": {\n",
    "                'name':'intuition',\n",
    "                'role':'input',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        \"Ne\": {\n",
    "                'name':'intuition',\n",
    "                'role':'input',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        \"Si\": {\n",
    "                'name':'sensing',\n",
    "                'role':'input',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        \"Se\": {\n",
    "                'name':'sensing',\n",
    "                'role':'input',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        'Ti': {\n",
    "                'name':'thinking',\n",
    "                'role':'output',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        'Te': {\n",
    "                'name':'thinking',\n",
    "                'role':'output',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        'Fi': {\n",
    "                'name':'feeling',\n",
    "                'role':'output',\n",
    "                'direction':'internal',\n",
    "                },\n",
    "        'Fe': {\n",
    "                'name':'feeling',\n",
    "                'role':'output',\n",
    "                'direction':'external'\n",
    "                }\n",
    "        }\n",
    "\n",
    "# Dictionary of pretrained models\n",
    "models = {\n",
    "        # Phase 1 models\n",
    "        'NiNe':None,\n",
    "        'NiSi':None,\n",
    "        'NiSe':None,\n",
    "        'NeSi':None,\n",
    "        'NeSe':None,\n",
    "        'SiSe':None,\n",
    "        \n",
    "        'TiTe':None,\n",
    "        'TiFi':None,\n",
    "        'TiFe':None,\n",
    "        'TeFi':None,\n",
    "        'TeFe':None,\n",
    "        'FiFe':None,\n",
    "            \n",
    "        # Phase 2 models\n",
    "    \n",
    "        # G1 Method (detect dominant cognitive function)\n",
    "        'NiTe':None,\n",
    "        'NiFe':None,\n",
    "        'SiTe':None,\n",
    "        'SiFe':None,\n",
    "        \n",
    "        'NeFi':None,\n",
    "        'NeTi':None,\n",
    "        'SeFi':None,\n",
    "        'SeTi':None,\n",
    "        \n",
    "        # G2 Method (detect correct direction)\n",
    "        'NiTi':None,\n",
    "        'NiFi':None,\n",
    "        'NeTe':None,\n",
    "        'NeFe':None,\n",
    "        \n",
    "        'SiTi':None,\n",
    "        'SiFi':None,\n",
    "        'SeTe':None,\n",
    "        'SeFe':None,\n",
    "        }\n",
    "\n",
    "# Supporter pretrained models, increases accuracy ~3%\n",
    "supporters = {\n",
    "        'NvS':None,\n",
    "        'FvT':None,\n",
    "        'IvE':None,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a06a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T18:59:23.278794Z",
     "iopub.status.busy": "2021-10-27T18:59:23.278125Z",
     "iopub.status.idle": "2021-10-27T18:59:24.915506Z",
     "shell.execute_reply": "2021-10-27T18:59:24.916018Z",
     "shell.execute_reply.started": "2021-10-27T18:28:24.245922Z"
    },
    "papermill": {
     "duration": 1.656323,
     "end_time": "2021-10-27T18:59:24.916228",
     "exception": false,
     "start_time": "2021-10-27T18:59:23.259905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read models\n",
    "path = \"../input/mbti-pretrained-models/\"\n",
    "\n",
    "for key in models:\n",
    "    with open(path + key + '.pickle', 'rb') as file:\n",
    "        models[key] = pickle.load(file)\n",
    "\n",
    "for _key in supporters:\n",
    "    with open(path + _key + '.pickle', 'rb') as file:\n",
    "        supporters[_key] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47cd7be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T18:59:25.136345Z",
     "iopub.status.busy": "2021-10-27T18:59:24.987768Z",
     "iopub.status.idle": "2021-10-27T18:59:25.168220Z",
     "shell.execute_reply": "2021-10-27T18:59:25.168777Z",
     "shell.execute_reply.started": "2021-10-27T18:50:10.462772Z"
    },
    "papermill": {
     "duration": 0.237118,
     "end_time": "2021-10-27T18:59:25.168956",
     "exception": false,
     "start_time": "2021-10-27T18:59:24.931838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the direction of a cognitive function (i => e | e => i)\n",
    "def flip_cf_direction(cognitiveFunction):\n",
    "    direction = cognitiveFunction[1]\n",
    "    new_direction = 'i' if direction == 'e' else 'e'\n",
    "    return cognitiveFunction[0] + new_direction\n",
    "\n",
    "# Pipeline for using a pretrained model to predict sample\n",
    "def process_classify_sample(modelObject, sample):\n",
    "    vectorizer = modelObject['cv']\n",
    "    label_encoder = modelObject['labelEncoder']\n",
    "    model = modelObject['model']\n",
    "    \n",
    "    # Preprocessing\n",
    "    clean_sample = clean(sample)\n",
    "    x = vectorizer.transform([clean_sample]).toarray()\n",
    "    \n",
    "    # Classification\n",
    "    y = model.predict(x)\n",
    "    y_probability = max(model.predict_proba(x)[0])\n",
    "    classified_cf = label_encoder.inverse_transform(y)[0]\n",
    "    return letters_to_functions(classified_cf), y_probability\n",
    "\n",
    "    \n",
    "# Phase 1 of classifying a personality type\n",
    "# Recognize the top perceiving cognitive function\n",
    "# and the top judging cognitive function\n",
    "def phase1(sample):\n",
    "    # INPUT => sample: string\n",
    "    # OUTPUT => input_cf_acc, output_cf_acc: Series \n",
    "\n",
    "    # Keep track of every input (perceiving) cognitive function likelihood\n",
    "    input_cf_acc = pd.Series({\"Ni\":0, \"Ne\":0, \"Si\":0, \"Se\":0}, dtype=float)\n",
    "    # same for output (judging) cognitive functions\n",
    "    output_cf_acc = pd.Series({\"Ti\":0, \"Te\":0, \"Fi\":0, \"Fe\":0}, dtype=float)\n",
    "    \n",
    "    input_cf = np.array(['Ni', 'Ne', 'Si', 'Se'])\n",
    "    output_cf = np.array(['Ti', 'Te', 'Fi', 'Fe'])\n",
    "    \n",
    "    # nested loop for input (perceiving) cognitive functions\n",
    "    # so models are: NiNe, NiSi, NiSe,  ...\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 4):\n",
    "            model_name = input_cf[i] + input_cf[j]\n",
    "            modelObject = models[model_name]\n",
    "            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "            \n",
    "            # Incrase likelihood of the prediction class\n",
    "            input_cf_acc[cognitive_fn] += probability\n",
    "            other_cf = input_cf[i] if input_cf[j] == cognitive_fn else input_cf[j]\n",
    "            \n",
    "            # Increase likelihood (smaller value) of other classes\n",
    "            input_cf_acc[other_cf] += 1 - probability\n",
    "            \n",
    "    # Another nested loop for output (judging) cognitive functions\n",
    "    # so models are: TiTe, TiFi, TiFe, ...\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 4):\n",
    "            model_name = output_cf[i] + output_cf[j]\n",
    "            modelObject = models[model_name]\n",
    "            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "            \n",
    "            # Incrase likelihood of the prediction class\n",
    "            output_cf_acc[cognitive_fn] += probability\n",
    "            other_cf = output_cf[i] if output_cf[j] == cognitive_fn else output_cf[j]\n",
    "            \n",
    "            # Increase likelihood (smaller value) of other classes\n",
    "            output_cf_acc[other_cf] += 1 - probability\n",
    "    \n",
    "    # Use supporter model (iNtuition vs Sensing)\n",
    "    modelObject = supporters[\"NvS\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase iNtuitive functions\n",
    "    if cognitive_fn == \"N\":\n",
    "        input_cf_acc[['Ni', 'Ne']] += probability\n",
    "        input_cf_acc[['Si', 'Se']] += 1 - probability\n",
    "    # Increase Sensing functions\n",
    "    else:\n",
    "        input_cf_acc[['Si', 'Se']] += probability\n",
    "        input_cf_acc[['Ni', 'Ne']] += 1 - probability\n",
    "    \n",
    "    # Use supporter model (Feeling vs Thinking)\n",
    "    modelObject = supporters[\"FvT\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase Feeling functions likelihood\n",
    "    if cognitive_fn == \"F\":\n",
    "        output_cf_acc[['Fi', 'Fe']] += probability\n",
    "        output_cf_acc[['Ti', 'Te']] += 1 - probability\n",
    "    # Increase Thinking Functions likelihood\n",
    "    else:\n",
    "        output_cf_acc[['Ti', 'Te']] += probability\n",
    "        output_cf_acc[['Fi', 'Fe']] += 1 - probability   \n",
    "    \n",
    "    # Use supporter model (Introvert vs Extrovert)\n",
    "    modelObject = supporters[\"IvE\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase Introverted functions likelihood\n",
    "    if cognitive_fn == \"I\":\n",
    "        input_cf_acc[['Ni', 'Si']] += probability\n",
    "        input_cf_acc[['Ne', 'Se']] += 1 - probability\n",
    "        \n",
    "        output_cf_acc[['Fi', 'Ti']] += probability\n",
    "        output_cf_acc[['Fe', 'Te']] += 1 - probability\n",
    "        \n",
    "    # Increase  Extroverted functions likelihood\n",
    "    else:\n",
    "        input_cf_acc[['Ne', 'Se']] += probability\n",
    "        input_cf_acc[['Ni', 'Si']] += 1 - probability\n",
    "        \n",
    "        output_cf_acc[['Fe', 'Te']] += probability\n",
    "        output_cf_acc[['Fi', 'Ti']] += 1 - probability\n",
    "    \n",
    "    # Return: likelihoods of perceiving (input) cognitive functions\n",
    "    # and judging (output) cogitive function\n",
    "    return input_cf_acc, output_cf_acc\n",
    "\n",
    "    \n",
    "    \n",
    "# Phase 2 of classification algorithm\n",
    "# Determine which cognitive function is the dominant\n",
    "# and which is the auxiliary.\n",
    "# and Fix the classification if necessary\n",
    "# Necessary: if phase 1 results a two cognitive functions\n",
    "# of the same direction (ex: Ni-Ti), this is not acceptable\n",
    "# since there's no personality with these Dom-Aux functions\n",
    "def phase2(sample, input_acc, output_acc):\n",
    "    # Number of classifications done on every cognitive functions\n",
    "    # (i.e. we ran 5 models having 'Ni' as one of it's classes)\n",
    "    counter_models_ran = 5\n",
    "    \n",
    "    # Get max-likelihood data (probability & className)\n",
    "    # maxInput is the perceiving function which got the maximum likelihood\n",
    "    # maxOutput is the judgning function ~~~~~~\n",
    "    maxInput = {'name':input_acc.idxmax(), 'proba':input_acc.max()}\n",
    "    maxOutput = {'name':output_acc.idxmax(), 'proba':output_acc.max()}\n",
    "    \n",
    "    # Get direction of each cognitive function\n",
    "    maxInputDirection = cf_info[maxInput['name']]['direction']\n",
    "    maxOutputDirection = cf_info[maxOutput['name']]['direction']\n",
    "    \n",
    "    # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n",
    "    cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n",
    "    phase2_model_name = \"\".join(cf_stack)\n",
    "    \n",
    "    # if both perceiving & judging classes (functions) are opposite direction\n",
    "    if maxInputDirection != maxOutputDirection:\n",
    "        # We know the top two cognitive functions (ie. Ni, Te)\n",
    "        # this path will run the appropriate models to\n",
    "        # determine which of these functions is Dominant (primary)\n",
    "        # and which is Auxiliary (secondary)\n",
    "\n",
    "        # determine which cognitive_function is the dominant one\n",
    "        modelObject = models[phase2_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "        \n",
    "    # both perceiving & judging functions have same direction (they must NOT)\n",
    "    else:\n",
    "        # There is an ambiguity, the top 2 cognitive_functions cannot be of\n",
    "        # the same direction (ie. Ni, Ti) (they're both 'i')\n",
    "        # One of them is correct, this function will detect which one is\n",
    "        \n",
    "        # Detect which of these functions is more accurate\n",
    "        modelObject = models[phase2_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "        \n",
    "        \n",
    "        # if dominant cognitive function is an input (perceiving) \n",
    "        # (ie: Ni, Ne, Si, Se)\n",
    "        # flip the direction of the other -output- function\n",
    "        # example: Ni-Ti  => Ni-Te\n",
    "        if dominant_cf_name == input_acc.idxmax():\n",
    "            problematic_cf = output_acc.idxmax()\n",
    "            input_acc[input_acc.idxmax()] += probability\n",
    "            fixed_cf_name = flip_cf_direction(problematic_cf)\n",
    "            temp_acc = output_acc[fixed_cf_name]\n",
    "            output_acc[fixed_cf_name] = output_acc[problematic_cf] + (1 - probability)\n",
    "            output_acc[problematic_cf] = temp_acc\n",
    "        \n",
    "        # else if the dominant cognitive function is an output (judging)\n",
    "        # (ie: Ti, Te, Fi, Fe)\n",
    "        # flip the direction of the other -input- function\n",
    "        # example: Ni-Ti => Ne-Ti\n",
    "        else:\n",
    "            problematic_cf = input_acc.idxmax()\n",
    "            output_acc[output_acc.idxmax()] += probability\n",
    "            fixed_cf_name = flip_cf_direction(problematic_cf)\n",
    "            temp_acc = input_acc[fixed_cf_name]\n",
    "            input_acc[fixed_cf_name] = input_acc[problematic_cf] + (1 - probability)\n",
    "            input_acc[problematic_cf] = temp_acc\n",
    "        \n",
    "        # Now we've fixed the direction of the (less) accurate cognitive function\n",
    "        # We need to determine which function is dominant\n",
    "        \n",
    "        # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n",
    "        cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n",
    "        phase2_final_model_name = \"\".join(cf_stack)\n",
    "\n",
    "        # Run model\n",
    "        modelObject = models[phase2_final_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "    \n",
    "    # Now we know which cognitive function is dominant and which is auxilary.\n",
    "    # handle their likelihoods\n",
    "    \n",
    "    # If dominant function is input (perceiving)\n",
    "    # increase its likelihood, decrease the other\n",
    "    if dominant_cf_name == input_acc.idxmax():\n",
    "        input_acc[input_acc.idxmax()] += probability\n",
    "        output_acc[output_acc.idxmax()] += 1 - probability\n",
    "\n",
    "    # else, dominant function is output (judging)\n",
    "    # increase its likelihood, decrease the other\n",
    "    else:\n",
    "        output_acc[output_acc.idxmax()] += probability\n",
    "        input_acc[input_acc.idxmax()] += 1 - probability\n",
    "\n",
    "    # Stack the cognitive functions as: Dominant,Auxiliary\n",
    "    cognitive_functions_stack = pd.Series({\n",
    "            input_acc.idxmax(): input_acc[input_acc.idxmax()],\n",
    "            output_acc.idxmax(): output_acc[output_acc.idxmax()]\n",
    "            })\n",
    "        \n",
    "    dominant_function = cognitive_functions_stack.idxmax()\n",
    "    auxiliary_function = cognitive_functions_stack.idxmin()\n",
    "    \n",
    "\n",
    "    # Convert cognitive functions to personality types\n",
    "    personality_type = functions_to_letters(dominant_function + auxiliary_function)\n",
    "    \n",
    "    # Calculate probability\n",
    "    probability = (cognitive_functions_stack[dominant_function] / counter_models_ran \n",
    "                   + cognitive_functions_stack[auxiliary_function] / counter_models_ran) / 2\n",
    "    \n",
    "    return personality_type, probability\n",
    "\n",
    "# Run classification algorithm phases\n",
    "def run(sample):\n",
    "    input_acc, output_acc = phase1(sample)\n",
    "    personality, probability = phase2(sample, input_acc, output_acc)\n",
    "    return personality, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9123b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T18:59:25.198190Z",
     "iopub.status.busy": "2021-10-27T18:59:25.197205Z",
     "iopub.status.idle": "2021-10-27T19:06:24.041732Z",
     "shell.execute_reply": "2021-10-27T19:06:24.042447Z",
     "shell.execute_reply.started": "2021-10-27T18:50:11.535247Z"
    },
    "papermill": {
     "duration": 418.861329,
     "end_time": "2021-10-27T19:06:24.042942",
     "exception": false,
     "start_time": "2021-10-27T18:59:25.181613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "validation_set = pd.read_csv(path + 'validation_set.csv')\n",
    "\n",
    "def classify(sentence):\n",
    "    personality, probability = run(sentence)\n",
    "    return personality\n",
    "\n",
    "# Results as 16 personality types\n",
    "y_pred = validation_set['posts'].apply(classify)\n",
    "y_true = validation_set['type']\n",
    "\n",
    "# Results as 4 categories (NT, NF, ST, SF)\n",
    "y_pred_soft = y_pred.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')\n",
    "y_true_soft = y_true.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "302eea9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T19:06:24.081357Z",
     "iopub.status.busy": "2021-10-27T19:06:24.080389Z",
     "iopub.status.idle": "2021-10-27T19:06:24.277660Z",
     "shell.execute_reply": "2021-10-27T19:06:24.278472Z",
     "shell.execute_reply.started": "2021-10-27T18:57:27.512691Z"
    },
    "papermill": {
     "duration": 0.220345,
     "end_time": "2021-10-27T19:06:24.278707",
     "exception": false,
     "start_time": "2021-10-27T19:06:24.058362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Classes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NF       0.84      0.83      0.84       649\n",
      "          NT       0.93      0.86      0.89      1202\n",
      "          SF       0.29      0.66      0.40        32\n",
      "          ST       0.54      0.77      0.63       117\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.65      0.78      0.69      2000\n",
      "weighted avg       0.87      0.84      0.85      2000\n",
      "\n",
      "\n",
      "16 Classes\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.00      0.00      0.00         0\n",
      "        ENFP       0.48      0.65      0.55        74\n",
      "        ENTJ       0.00      0.00      0.00         0\n",
      "        ENTP       0.61      0.65      0.63       137\n",
      "        ESFJ       0.00      0.00      0.00         0\n",
      "        ESFP       0.33      0.33      0.33         3\n",
      "        ESTJ       0.00      0.00      0.00         0\n",
      "        ESTP       0.72      0.90      0.80        29\n",
      "        INFJ       0.79      0.70      0.74       310\n",
      "        INFP       0.77      0.73      0.75       265\n",
      "        INTJ       0.84      0.69      0.76       502\n",
      "        INTP       0.85      0.80      0.83       563\n",
      "        ISFJ       0.28      0.67      0.39        12\n",
      "        ISFP       0.19      0.35      0.25        17\n",
      "        ISTJ       0.32      0.65      0.43        17\n",
      "        ISTP       0.47      0.59      0.53        71\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.42      0.48      0.44      2000\n",
      "weighted avg       0.77      0.72      0.74      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports\n",
    "print(\"4 Classes\")\n",
    "print(classification_report(y_true_soft, y_pred_soft))\n",
    "\n",
    "print(\"\\n16 Classes\\n\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad36e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T19:06:24.310085Z",
     "iopub.status.busy": "2021-10-27T19:06:24.309118Z",
     "iopub.status.idle": "2021-10-27T19:06:24.360419Z",
     "shell.execute_reply": "2021-10-27T19:06:24.361151Z",
     "shell.execute_reply.started": "2021-10-27T18:57:33.002115Z"
    },
    "papermill": {
     "duration": 0.068407,
     "end_time": "2021-10-27T19:06:24.361573",
     "exception": false,
     "start_time": "2021-10-27T19:06:24.293166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ENTJ\n",
      "Likelihood: 0.6912121287040548\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey, they call me the Commander, I create plans and strategies for everything,\n",
    "I solve problems using highly optimized solutions, and I use my intuition\n",
    "to predict possible scenarios in the future, some people consider me as bossy,\n",
    "but I'm not, regonized me?\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ENTJ\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed4dc78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T19:06:24.398754Z",
     "iopub.status.busy": "2021-10-27T19:06:24.393013Z",
     "iopub.status.idle": "2021-10-27T19:06:24.453168Z",
     "shell.execute_reply": "2021-10-27T19:06:24.454134Z",
     "shell.execute_reply.started": "2021-10-27T18:57:35.413889Z"
    },
    "papermill": {
     "duration": 0.077787,
     "end_time": "2021-10-27T19:06:24.454374",
     "exception": false,
     "start_time": "2021-10-27T19:06:24.376587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ISFP\n",
      "Likelihood: 0.6744017846686068\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey, they call me the Adventurer, I do artworks and sometimes play guitar,\n",
    "I enjoy spending time alone listening to music, I appreciate authenticity\n",
    "and honesty, I'm always connected to my feelings and alert of it,\n",
    "some people consider as an artist, but I'm not, and I won't let them\n",
    "define what I am, regonized me?\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ISFP\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af453ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T19:06:24.492586Z",
     "iopub.status.busy": "2021-10-27T19:06:24.491418Z",
     "iopub.status.idle": "2021-10-27T19:06:24.532816Z",
     "shell.execute_reply": "2021-10-27T19:06:24.533343Z",
     "shell.execute_reply.started": "2021-10-27T18:57:39.286373Z"
    },
    "papermill": {
     "duration": 0.064072,
     "end_time": "2021-10-27T19:06:24.533516",
     "exception": false,
     "start_time": "2021-10-27T19:06:24.469444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ESTP\n",
      "Likelihood: 0.7212842011608569\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey guys, wanna hang out ? come on ..\n",
    "hiking out, flirting with girls, going to parties ..\n",
    "what's life without fun!\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ESxP\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0808ca48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T19:06:24.565436Z",
     "iopub.status.busy": "2021-10-27T19:06:24.564791Z",
     "iopub.status.idle": "2021-10-27T19:06:24.627782Z",
     "shell.execute_reply": "2021-10-27T19:06:24.628833Z",
     "shell.execute_reply.started": "2021-10-27T18:57:41.495441Z"
    },
    "papermill": {
     "duration": 0.081506,
     "end_time": "2021-10-27T19:06:24.629108",
     "exception": false,
     "start_time": "2021-10-27T19:06:24.547602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: INTP\n",
      "Likelihood: 0.6571975312658572\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey. .. .. .. what? .. .. ..\n",
    "okay I forgot. Hi, I'm the Logician, or at least that's what they call me besides 'the robot',\n",
    "I use my critical thinking skills to logically analyze everything,\n",
    "I spend a lot of my time thinking about solutions for problems that will never occur,\n",
    "reading articles, investigating theories, understanding concepts, it's all about my mind.\n",
    "I know I'm in a computer program being tested, I recognized you ..\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # INTP\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4511b",
   "metadata": {
    "papermill": {
     "duration": 0.014543,
     "end_time": "2021-10-27T19:06:24.659570",
     "exception": false,
     "start_time": "2021-10-27T19:06:24.645027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 433.753038,
   "end_time": "2021-10-27T19:06:25.890325",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-27T18:59:12.137287",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
